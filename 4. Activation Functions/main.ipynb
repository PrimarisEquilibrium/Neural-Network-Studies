{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "nnfs.init()\n",
    "from nnfs.datasets import spiral_data\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = np.maximum(0, inputs)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.00299556 0.00964661]\n",
      " [0.01288097 0.01556285]\n",
      " [0.02997479 0.0044481 ]\n",
      " [0.03931246 0.00932828]]\n"
     ]
    }
   ],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "dense1.forward(X)\n",
    "\n",
    "print(dense1.output[:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "E = math.e\n",
    "\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E ** output)\n",
    "print('exponentiated values:')\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized exponentiated values:\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "Sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Now normalize values\n",
    "norm_base = sum(exp_values) # We sum all values\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print('Normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "print('Sum of normalized values:', sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "normalized exponentiated values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Values from the earlier previous when we described\n",
    "# what a neural network is\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "\n",
    "# Now normalize values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print('normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "print('sum of normalized values:', np.sum(norm_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 15])"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([\n",
    "    [1, 2, 3], \n",
    "    [4, 5, 6]\n",
    "]))\n",
    "\n",
    "np.sum(np.array([\n",
    "    [1, 2, 3], \n",
    "    [4, 5, 6]\n",
    "]), axis=0) # 1 x 3\n",
    "\n",
    "np.sum(np.array([\n",
    "    [1, 2, 3], \n",
    "    [4, 5, 6]\n",
    "]), axis=1) # 2 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum without axis\n",
      "18.172\n",
      "Another way to think of it w/ a matrix == axis 0: columns:\n",
      "[15.11   0.451  2.611]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = np.array(\n",
    "    [[4.8, 1.21, 2.385],\n",
    "     [8.9, -1.81, 0.2],\n",
    "     [1.41, 1.051, 0.026]]\n",
    ")\n",
    "\n",
    "print('Sum without axis')\n",
    "print(np.sum(layer_outputs))\n",
    "\n",
    "print('Another way to think of it w/ a matrix == axis 0: columns:')\n",
    "print(np.sum(layer_outputs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But we want to sum the rows instead, like this w/ raw py:\n",
      "8.395\n",
      "7.29\n",
      "2.4869999999999997\n",
      "So we can sum axis 1, but note the current shape:\n",
      "[8.395 7.29  2.487]\n",
      "Sum axis 1, but keep the same dimensions as input:\n",
      "[[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n"
     ]
    }
   ],
   "source": [
    "print('But we want to sum the rows instead, like this w/ raw py:')\n",
    "for i in layer_outputs:\n",
    "    print(sum(i))\n",
    "\n",
    "print('So we can sum axis 1, but note the current shape:')\n",
    "print(np.sum(layer_outputs, axis=1))\n",
    "\n",
    "print('Sum axis 1, but keep the same dimensions as input:')\n",
    "print(np.sum(layer_outputs, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "]\n",
    "\n",
    "X_exp_values = np.exp(X)\n",
    "X_exp_sum = np.sum(X_exp_values, axis=1, keepdims=True)\n",
    "output = X_exp_values / X_exp_sum\n",
    "\n",
    "row_sums = np.sum(output, axis=1, keepdims=True)\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333343, 0.33333343, 0.33333316],\n",
       "       [0.33333343, 0.33333343, 0.3333331 ],\n",
       "       [0.33333352, 0.33333352, 0.333333  ],\n",
       "       [0.3333337 , 0.3333336 , 0.33333266],\n",
       "       [0.33333385, 0.33333373, 0.33333242],\n",
       "       [0.33333358, 0.33333352, 0.33333287],\n",
       "       [0.33333403, 0.33333388, 0.3333321 ],\n",
       "       [0.33333394, 0.33333382, 0.3333322 ],\n",
       "       [0.33333403, 0.33333388, 0.3333321 ],\n",
       "       [0.33333433, 0.33333415, 0.33333153],\n",
       "       [0.33333403, 0.33333388, 0.33333212],\n",
       "       [0.33333457, 0.3333343 , 0.33333117],\n",
       "       [0.33333442, 0.3333342 , 0.33333135],\n",
       "       [0.3333339 , 0.3333338 , 0.3333323 ],\n",
       "       [0.33333483, 0.33333454, 0.3333306 ],\n",
       "       [0.33333382, 0.3333337 , 0.33333248],\n",
       "       [0.33333454, 0.33333427, 0.3333312 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333403, 0.33333388, 0.3333321 ],\n",
       "       [0.3333341 , 0.33333397, 0.33333194],\n",
       "       [0.33333448, 0.33333424, 0.33333132],\n",
       "       [0.33333343, 0.33333343, 0.33333316],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333126, 0.33333385, 0.3333349 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333278, 0.33333346, 0.33333376],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333206, 0.33333364, 0.33333433],\n",
       "       [0.3333317 , 0.33333373, 0.33333457],\n",
       "       [0.33333012, 0.33333415, 0.33333576],\n",
       "       [0.33332986, 0.3333342 , 0.33333594],\n",
       "       [0.33333012, 0.33333412, 0.33333573],\n",
       "       [0.3333295 , 0.33333433, 0.33333617],\n",
       "       [0.3333295 , 0.33333433, 0.3333362 ],\n",
       "       [0.33332944, 0.33333433, 0.33333623],\n",
       "       [0.33332923, 0.33333436, 0.33333635],\n",
       "       [0.33332917, 0.3333344 , 0.3333364 ],\n",
       "       [0.33332908, 0.33333445, 0.33333647],\n",
       "       [0.33332896, 0.33333448, 0.3333366 ],\n",
       "       [0.33332908, 0.33333445, 0.3333365 ],\n",
       "       [0.33332935, 0.3333344 , 0.33333626],\n",
       "       [0.33332866, 0.3333345 , 0.33333677],\n",
       "       [0.3333322 , 0.33333522, 0.33333257],\n",
       "       [0.33332923, 0.33333442, 0.33333635],\n",
       "       [0.33333042, 0.33333465, 0.33333495],\n",
       "       [0.3333334 , 0.33333573, 0.3333309 ],\n",
       "       [0.33333507, 0.33333623, 0.3333287 ],\n",
       "       [0.33332834, 0.33333465, 0.333337  ],\n",
       "       [0.3333329 , 0.3333357 , 0.33333138],\n",
       "       [0.33333832, 0.33333734, 0.3333243 ],\n",
       "       [0.3333286 , 0.3333346 , 0.33333683],\n",
       "       [0.33333835, 0.33333734, 0.33332428],\n",
       "       [0.33332825, 0.33333465, 0.3333371 ],\n",
       "       [0.3333339 , 0.33333623, 0.33332986],\n",
       "       [0.333339  , 0.3333379 , 0.33332306],\n",
       "       [0.33333266, 0.33333594, 0.3333314 ],\n",
       "       [0.33333755, 0.33333725, 0.3333252 ],\n",
       "       [0.33333606, 0.33333695, 0.333327  ],\n",
       "       [0.33333838, 0.33333737, 0.33332428],\n",
       "       [0.3333402 , 0.33333886, 0.33332098],\n",
       "       [0.3333398 , 0.33333853, 0.33332172],\n",
       "       [0.33334038, 0.33333904, 0.33332062],\n",
       "       [0.33334047, 0.33333904, 0.3333205 ],\n",
       "       [0.33334005, 0.33333874, 0.33332115],\n",
       "       [0.33334023, 0.3333389 , 0.33332092],\n",
       "       [0.33334062, 0.33333918, 0.3333202 ],\n",
       "       [0.33333644, 0.33333585, 0.33332774],\n",
       "       [0.3333347 , 0.33333445, 0.3333309 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333343 , 0.33333412, 0.3333316 ],\n",
       "       [0.33333874, 0.3333377 , 0.3333235 ],\n",
       "       [0.3333397 , 0.33333844, 0.33332184],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33334005, 0.33333874, 0.3333212 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333477, 0.33333448, 0.33333075],\n",
       "       [0.33333164, 0.3333337 , 0.33333465],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333262 , 0.33333513, 0.33333865],\n",
       "       [0.33332878, 0.33333445, 0.33333677],\n",
       "       [0.33332917, 0.33333436, 0.3333365 ],\n",
       "       [0.33332923, 0.33333433, 0.33333647],\n",
       "       [0.3333247 , 0.33333552, 0.33333975],\n",
       "       [0.33332425, 0.33333567, 0.33334005],\n",
       "       [0.33332485, 0.3333355 , 0.33333966],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333332 , 0.33333337, 0.33333346],\n",
       "       [0.3333331 , 0.33333337, 0.3333335 ],\n",
       "       [0.33333302, 0.33333343, 0.3333336 ],\n",
       "       [0.33333287, 0.33333343, 0.33333367],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333275, 0.3333335 , 0.3333338 ],\n",
       "       [0.33333257, 0.33333352, 0.33333385],\n",
       "       [0.3333325 , 0.33333352, 0.33333394],\n",
       "       [0.33333254, 0.33333352, 0.3333339 ],\n",
       "       [0.33333257, 0.33333352, 0.3333339 ],\n",
       "       [0.33333233, 0.33333358, 0.33333406],\n",
       "       [0.33333212, 0.33333367, 0.33333424],\n",
       "       [0.33333233, 0.3333336 , 0.33333406],\n",
       "       [0.3333332 , 0.33333394, 0.33333287],\n",
       "       [0.33333233, 0.33333367, 0.33333397],\n",
       "       [0.33333373, 0.33333418, 0.33333203],\n",
       "       [0.33333296, 0.333334  , 0.33333305],\n",
       "       [0.33333248, 0.33333388, 0.33333364],\n",
       "       [0.33333176, 0.33333376, 0.3333345 ],\n",
       "       [0.3333337 , 0.33333436, 0.33333194],\n",
       "       [0.33333462, 0.33333462, 0.33333075],\n",
       "       [0.33333552, 0.3333351 , 0.33332944],\n",
       "       [0.33333492, 0.33333477, 0.33333027],\n",
       "       [0.33333567, 0.33333522, 0.3333291 ],\n",
       "       [0.3333341 , 0.33333468, 0.33333123],\n",
       "       [0.33333498, 0.33333492, 0.33333004],\n",
       "       [0.333336  , 0.33333546, 0.33332857],\n",
       "       [0.33333543, 0.33333513, 0.33332944],\n",
       "       [0.33333623, 0.33333564, 0.3333281 ],\n",
       "       [0.33333638, 0.3333358 , 0.33332783],\n",
       "       [0.33333647, 0.33333588, 0.33332765],\n",
       "       [0.33333647, 0.33333588, 0.3333277 ],\n",
       "       [0.33333424, 0.3333341 , 0.33333173],\n",
       "       [0.33333683, 0.33333614, 0.33332703],\n",
       "       [0.33333632, 0.33333576, 0.3333279 ],\n",
       "       [0.3333362 , 0.33333564, 0.33332813],\n",
       "       [0.33333632, 0.33333576, 0.3333279 ],\n",
       "       [0.3333367 , 0.33333606, 0.33332726],\n",
       "       [0.33333704, 0.33333632, 0.33332664],\n",
       "       [0.33333626, 0.3333357 , 0.33332804],\n",
       "       [0.3333349 , 0.3333346 , 0.3333305 ],\n",
       "       [0.33333585, 0.33333537, 0.33332878],\n",
       "       [0.33333302, 0.33333337, 0.3333336 ],\n",
       "       [0.33333609, 0.33333555, 0.33332843],\n",
       "       [0.3333354 , 0.33333498, 0.33332962],\n",
       "       [0.33333758, 0.33333674, 0.33332565],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333368 , 0.3333361 , 0.3333271 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333033, 0.33333406, 0.33333555],\n",
       "       [0.33333147, 0.33333376, 0.33333477],\n",
       "       [0.33333027, 0.33333406, 0.33333564],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333297 , 0.33333424, 0.33333603],\n",
       "       [0.3333306 , 0.333334  , 0.3333354 ],\n",
       "       [0.3333295 , 0.3333343 , 0.3333362 ],\n",
       "       [0.33332863, 0.33333454, 0.33333686],\n",
       "       [0.33333254, 0.3333335 , 0.33333394],\n",
       "       [0.3333307 , 0.333334  , 0.33333534],\n",
       "       [0.33332783, 0.3333347 , 0.33333743],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333274 , 0.33333486, 0.33333772],\n",
       "       [0.33332732, 0.33333486, 0.33333778],\n",
       "       [0.3333276 , 0.3333348 , 0.3333376 ],\n",
       "       [0.3333271 , 0.33333492, 0.33333796],\n",
       "       [0.33332837, 0.33333457, 0.33333704],\n",
       "       [0.33332703, 0.33333495, 0.33333802],\n",
       "       [0.33332783, 0.3333348 , 0.33333743],\n",
       "       [0.33332908, 0.33333504, 0.3333359 ],\n",
       "       [0.33333904, 0.3333379 , 0.33332303],\n",
       "       [0.33332676, 0.33333504, 0.3333382 ],\n",
       "       [0.3333264 , 0.33333513, 0.33333847],\n",
       "       [0.33332652, 0.33333513, 0.33333838],\n",
       "       [0.33332625, 0.33333516, 0.33333862],\n",
       "       [0.33332777, 0.3333348 , 0.33333743],\n",
       "       [0.33333206, 0.33333638, 0.33333156],\n",
       "       [0.33333054, 0.3333359 , 0.33333355],\n",
       "       [0.33333978, 0.33333847, 0.33332178],\n",
       "       [0.33333194, 0.33333647, 0.33333158],\n",
       "       [0.33333254, 0.3333367 , 0.3333307 ],\n",
       "       [0.3333323 , 0.33333668, 0.333331  ],\n",
       "       [0.33332694, 0.33333504, 0.33333805],\n",
       "       [0.33332798, 0.33333522, 0.33333677],\n",
       "       [0.33333233, 0.33333683, 0.33333078],\n",
       "       [0.3333422 , 0.33334047, 0.3333174 ],\n",
       "       [0.33334234, 0.3333406 , 0.3333171 ],\n",
       "       [0.33334187, 0.33334023, 0.33331794],\n",
       "       [0.33333752, 0.3333384 , 0.33332404],\n",
       "       [0.3333341 , 0.33333758, 0.3333283 ],\n",
       "       [0.3333422 , 0.33334047, 0.33331743],\n",
       "       [0.3333427 , 0.33334085, 0.33331636],\n",
       "       [0.3333421 , 0.3333404 , 0.33331752],\n",
       "       [0.3333401 , 0.3333388 , 0.33332112],\n",
       "       [0.33334187, 0.33334023, 0.33331794],\n",
       "       [0.33333552, 0.33333507, 0.33332938],\n",
       "       [0.33334228, 0.33334053, 0.3333172 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333343, 0.33333343, 0.33333316],\n",
       "       [0.33333352, 0.3333335 , 0.33333296],\n",
       "       [0.3333335 , 0.33333346, 0.33333305],\n",
       "       [0.33333367, 0.33333364, 0.3333327 ],\n",
       "       [0.33333334, 0.33333334, 0.3333333 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333406, 0.3333339 , 0.33333206],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333418, 0.33333403, 0.33333182],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333376, 0.33333367, 0.3333326 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333302, 0.3333334 , 0.33333358],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333305, 0.3333334 , 0.33333358],\n",
       "       [0.3333315 , 0.33333382, 0.3333347 ],\n",
       "       [0.33333275, 0.3333335 , 0.3333338 ],\n",
       "       [0.3333326 , 0.3333335 , 0.3333339 ],\n",
       "       [0.33333117, 0.3333339 , 0.33333495],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333084, 0.333334  , 0.3333352 ],\n",
       "       [0.33333173, 0.3333337 , 0.33333454],\n",
       "       [0.33333084, 0.33333397, 0.33333516],\n",
       "       [0.3333307 , 0.333334  , 0.33333528],\n",
       "       [0.3333305 , 0.3333341 , 0.33333543],\n",
       "       [0.3333305 , 0.33333406, 0.33333546],\n",
       "       [0.3333305 , 0.33333406, 0.3333354 ],\n",
       "       [0.33333084, 0.33333403, 0.3333352 ],\n",
       "       [0.3333318 , 0.3333337 , 0.33333454],\n",
       "       [0.33333007, 0.33333418, 0.33333576],\n",
       "       [0.33333012, 0.33333415, 0.33333576],\n",
       "       [0.33332983, 0.33333424, 0.33333597],\n",
       "       [0.333336  , 0.3333357 , 0.3333283 ],\n",
       "       [0.33333024, 0.33333415, 0.3333356 ],\n",
       "       [0.33333117, 0.3333344 , 0.33333448],\n",
       "       [0.33332944, 0.33333433, 0.33333623],\n",
       "       [0.33332992, 0.33333424, 0.33333585],\n",
       "       [0.3333333 , 0.33333525, 0.33333144],\n",
       "       [0.3333347 , 0.3333357 , 0.33332956],\n",
       "       [0.33333656, 0.33333614, 0.3333273 ],\n",
       "       [0.33333504, 0.33333588, 0.3333291 ],\n",
       "       [0.33333737, 0.3333366 , 0.33332604],\n",
       "       [0.33333763, 0.3333368 , 0.3333256 ],\n",
       "       [0.3333298 , 0.33333427, 0.3333359 ],\n",
       "       [0.33333778, 0.33333692, 0.33332533],\n",
       "       [0.3333386 , 0.33333758, 0.33332384],\n",
       "       [0.33333868, 0.33333766, 0.33332366],\n",
       "       [0.3333383 , 0.3333373 , 0.33332437],\n",
       "       [0.33333808, 0.33333716, 0.33332476],\n",
       "       [0.33333796, 0.33333707, 0.3333249 ],\n",
       "       [0.3333391 , 0.33333796, 0.3333229 ],\n",
       "       [0.33333772, 0.3333369 , 0.3333254 ],\n",
       "       [0.33333772, 0.333337  , 0.33332524],\n",
       "       [0.33333942, 0.33333826, 0.33332238],\n",
       "       [0.33333948, 0.3333383 , 0.33332226],\n",
       "       [0.33333698, 0.33333626, 0.33332673],\n",
       "       [0.33333927, 0.3333381 , 0.33332264],\n",
       "       [0.33333895, 0.33333784, 0.33332318],\n",
       "       [0.33333814, 0.33333722, 0.33332467],\n",
       "       [0.33333585, 0.33333537, 0.33332878],\n",
       "       [0.33333772, 0.3333369 , 0.33332542],\n",
       "       [0.3333398 , 0.33333853, 0.33332172],\n",
       "       [0.33333358, 0.33333352, 0.3333329 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333486, 0.33333457, 0.33333063],\n",
       "       [0.3333371 , 0.33333635, 0.33332652],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333339 , 0.3333338 , 0.33333233],\n",
       "       [0.33333224, 0.33333355, 0.33333424],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333135, 0.33333382, 0.33333486],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333296, 0.33333337, 0.3333337 ],\n",
       "       [0.3333303 , 0.33333406, 0.33333564],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33332807, 0.33333465, 0.33333728],\n",
       "       [0.33332896, 0.33333442, 0.33333662],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333087, 0.33333394, 0.33333525],\n",
       "       [0.33332503, 0.33333546, 0.33333948],\n",
       "       [0.3333248 , 0.33333555, 0.3333397 ],\n",
       "       [0.3333253 , 0.3333354 , 0.33333933],\n",
       "       [0.3333246 , 0.33333558, 0.3333398 ],\n",
       "       [0.33332452, 0.3333356 , 0.3333399 ],\n",
       "       [0.3333245 , 0.3333356 , 0.33333984],\n",
       "       [0.33332685, 0.33333528, 0.33333784],\n",
       "       [0.33332562, 0.33333528, 0.3333391 ],\n",
       "       [0.33332586, 0.33333528, 0.3333388 ]], dtype=float32)"
      ]
     },
     "execution_count": 841,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A neural network dense layer (each neuron is connected to all the other neurons in the next layer)\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Define weights for this layer\n",
    "        #  - Scale by 0.01 to be proportional for small training adjustments later\n",
    "        #  - The dimensions of a weight matrix is W = [n^[l-1] x n^[l]]\n",
    "        #    - Each row represents a weight set for each feature/neuron in the previous layer\n",
    "        #    - Each nth column represents the weights that connect to the nth neuron\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "\n",
    "        # Define biases for this layer\n",
    "        #  - Initialize as 0\n",
    "        #  - The dimensions of a bias matrix is b = [1 x n^[l]]\n",
    "        #  - Each column represents the bias for each neuron\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Inputs is a matrix [#of samples x #of inputs]\n",
    "        # Weights is [#of inputs x #of neurons]\n",
    "        #  - Row and column of inputs and weights match, therefore matrix multiplication is possible\n",
    "        # Biases is automatically broadcasted to fix multiple samples\n",
    "        return inputs @ self.weights + self.biases\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Inputs is a matrix [#of samples x #of input/neurons]\n",
    "        # ReLU is max(0, inputs)\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "# Softmax takes non-normalized data and produces a probability distrubtion\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Inputs is a matrix [#of samples x #of input/neurons]\n",
    "\n",
    "        # Take the exponential of the inputs matrix subtracted the vector column\n",
    "        # containing the max value of each row\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        # Take the sum of all rows of the exp matrix\n",
    "        exp_sum = np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        return exp_values / exp_sum\n",
    "\n",
    "\n",
    "layer1 = Layer_Dense(2, 3)\n",
    "layer2 = Layer_Dense(3, 3)\n",
    "ReLU = Activation_ReLU()\n",
    "SoftMax = Activation_Softmax()\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "z1 = layer1.forward(X)\n",
    "a1 = ReLU.forward(z1)\n",
    "\n",
    "z2 = layer2.forward(a1)\n",
    "y_hat = SoftMax.forward(z2)\n",
    "y_hat # Our model is completely random\n",
    "\n",
    "# The next step is to quantify how wrong the model is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neural-Network-SQH2Qv-A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
